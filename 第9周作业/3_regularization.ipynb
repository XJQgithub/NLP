{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (18724, 28, 28) (18724,)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (18724, 784) (18724, 10)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + beta_regul * tf.nn.l2_loss(weights)\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\672\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 17.703466\n",
      "Minibatch accuracy: 15.6%\n",
      "Validation accuracy: 21.2%\n",
      "Minibatch loss at step 500: 2.648446\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 1000: 1.686725\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 1500: 1.535719\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 2000: 1.049183\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 2500: 0.935073\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 3000: 0.695417\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 81.8%\n",
      "Test accuracy: 88.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEMCAYAAAAoB2Y1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nK6WUD9Gkr5RSPkSTvlJK+ZD/B8+IVZFmHK9oAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 399.890564\n",
      "Minibatch accuracy: 3.9%\n",
      "Validation accuracy: 26.7%\n",
      "Minibatch loss at step 10: 40.911041\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 71.6%\n",
      "Minibatch loss at step 20: 0.286050\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 30: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 40: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 50: 0.000002\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 60: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 70: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 80: 0.000001\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 90: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Test accuracy: 81.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = ((step % num_batches) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 10 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "  logits = tf.matmul(drop1, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 509.036865\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 31.9%\n",
      "Minibatch loss at step 10: 0.287808\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 68.5%\n",
      "Minibatch loss at step 20: 0.538404\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 30: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.7%\n",
      "Minibatch loss at step 40: 0.167570\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 71.2%\n",
      "Minibatch loss at step 50: 0.185100\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 71.0%\n",
      "Minibatch loss at step 60: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.6%\n",
      "Minibatch loss at step 70: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.6%\n",
      "Minibatch loss at step 80: 1.056802\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 70.9%\n",
      "Minibatch loss at step 90: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.6%\n",
      "Test accuracy: 78.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_batches\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 10 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 256\n",
    "num_hidden_nodes3 = 128\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "  weights4 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  lay3_train = tf.nn.relu(tf.matmul(lay2_train, weights3) + biases3)\n",
    "  logits = tf.matmul(lay3_train, weights4) + biases4\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 4000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.405918\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 31.4%\n",
      "Test accuracy: 90.3%\n"
     ]
    }
   ],
   "source": [
    "um_steps = 18001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
